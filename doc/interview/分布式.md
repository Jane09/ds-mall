### Dubbo的底层实现原理和机制


### 为什么不用 zookeeper？



### 需要生成全局唯一的递增 ID, 并发量非常大, 怎么做
    snowflake
        snowflake 无法实现全局递增，只能实现全局唯一，单机递增
    似 TDDL 那样一次取一个 ID 段，放在本地慢慢分配的策略




## 限流
   [参考](https://manzhizhen.iteye.com/blog/2311691)

### 限流功能实现方法
    令牌桶

    通过限制单位时间段内调用量来限流
        第一步我们需要做的就是确定这个单位时间段有多长，肯定不能太长，太长将会导致限流的效果变得不够“敏感”，如果单位时间段设置得太短，越短的单位时间段将导致我们的阈值越难设置
            最优的单位时间片段应该以阈值设置的难易程度为标准，比如我们的监控系统统计的是服务每分钟的调用量，所以很自然我们可以选择1分钟作为时间片段，
        考虑计数器的实现了
            AtomicLong


    通过限制系统的并发调用程度来限流
    使用漏桶（Leaky Bucket）算法来进行限流
    使用令牌桶（Token Bucket）算法来进行限流
        桶中会有一定数量的令牌，每次请求调用需要去桶中拿取一个令牌，拿到令牌后才有资格执行请求调用，否则只能等待能拿到足够的令牌数
        令牌桶算法的精髓就在于“拿令牌”和“放令牌”的方式，这和单纯的并发量限流有明显区别
        令牌桶算法不同，因为每次请求获取的令牌数不是固定的，比如当桶中的令牌数还比较多时，每次调用只需要获取一个令牌，
        随着桶中的令牌数逐渐减少，当到令牌的使用率（即使用中的令牌数/令牌总数）达某个比例，可能一次请求需要获取两个令牌，当令牌使用率到了一个更高的比例，可能一次请求调用需要获取更多的令牌数。

        同时，当调用使用完令牌后，有两种令牌生成方法，第一种就是直接往桶中放回使用的令牌数，第二种就是不做任何操作，有另一个额外的令牌生成步骤来将令牌匀速放回桶中。

        RateLimter 令牌桶

### 限流要做成分布式的, 怎么做?
    令牌桶维护到 Redis 里，每个实例起一个线程抢锁，抢到锁的负责定时放令牌
### 怎么抢锁?
    Redis setnx
### 锁怎么释放?
    抢到锁后设置过期时间，线程本身退出时主动释放锁，
    假如线程卡住了，锁过期那么其它线程可以继续抢占
### 加了超时之后有没有可能在没有释放的情况下, 被人抢走锁
    有可能，单次处理时间过长，锁泄露
### 怎么解决?
    换 zk，用心跳解决
### 不用 zk 的心跳, 可以怎么解决这个问题呢?
    每次更新过期时间时，Redis 用 MULTI 做 check-and-set 检查更新时间是否被其他线程修改了，假如被修改了，说明锁已经被抢走，放弃这把锁
### 假如这个限流希望做成可配置的, 需要有一个后台管理系统随意对某个 api 配置全局流量, 怎么做？
    在 Redis 里存储每个 API 的令牌桶 key，假如存在这个 key，则需要按上述逻辑进行限流





## 分布式数据一致性
### 数据库一致性解决方案

#### 规避分布式事务-业务整合
    缺点：显而易见，把本来规划拆分好的业务，又耦合到了一起，业务职责不清晰，不利于维护。
#### eBay模式（最终一致性 BASE）
    basically available, soft state, eventually consistent
    将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。
    人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理

   ![ebay](../images/base.jpeg)

    在第一阶段，通过本地的数据库的事务保障，增加了 transaction 表及消息队列 。
    在第二阶段，分别读出消息队列（但不删除），通过判断更新记录表 updates_applied
        来检测相关记录是否被执行，未被执行的记录会修改 user 表，然后增加一条操作记录到
        updates_applied，事务执行成功之后再删除队列。

#### 去哪儿分布式事务方案
    将分布式事务转换为多个本地事务，然后依靠重试等方式达到最终一致性

#### 蘑菇街
    准实时的最终一致性
    过多依赖其他服务

   ![蘑菇街](../images/mgj.jpeg)

#### 支付宝及蚂蚁金服DTS方案
    Distributed Transaction Service, DTS
    DTS 从架构上分为 xts-client 和 xts-server 两部分，前者是一个嵌入客户端应用的 JAR 包，主要负责事务数据的写入和处理；
    后者是一个独立的系统，主要负责异常事务的恢复

    最终一致：事务处理过程中，会有短暂不一致的情况，但通过恢复系统，可以让事务的数据达到最终一致的目标。
    协议简单：DTS 定义了类似 2PC 的标准两阶段接口，业务系统只需要实现对应的接口就可以使用 DTS 的事务功能。
    与 RPC 服务协议无关：在 SOA 架构下，一个或多个 DB 操作往往被包装成一个一个的 Service，Service 与 Service 之间通过 RPC 协议通信。DTS 框架构建在 SOA 架构上，与底层协议无关。
    与底层事务实现无关： DTS 是一个抽象的基于 Service 层的概念，与底层事务实现无关，也就是说在 DTS 的范围内，无论是关系型数据库 MySQL，Oracle，还是 KV 存储 MemCache，或者列存数据库 HBase，只要将对其的操作包装成 DTS 的参与者，就可以接入到 DTS 事务范围内。

   ![DTS](../images/dts.jpeg)

    实现
    一个完整的业务活动由一个主业务服务与若干从业务服务组成。
    主业务服务负责发起并完成整个业务活动。
    从业务服务提供 TCC 型业务操作。
    业务活动管理器控制业务活动的一致性，它登记业务活动中的操作，并在活动提交时确认所有的两阶段事务的 confirm 操作，在业务活动取消时调用所有两阶段事务的 cancel 操作。”

    与 2PC 协议比较
    没有单独的 Prepare 阶段，降低协议成本
    系统故障容忍度高，恢复简单

#### 农信网方案


### ID生成系统

   [美团ID生成ID](https://tech.meituan.com/2017/04/21/mt-leaf.html)

    全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。
    趋势递增：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。
    单调递增：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求。
    信息安全：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；
        如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则。
    要求：
    平均延迟和TP999延迟都要尽可能低；
    可用性5个9；
    高QPS

    UUID
        优点：
        性能非常高：本地生成，没有网络消耗。
        缺点：
        不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。
        信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
        ① MySQL官方有明确的建议主键要尽量越短越好[4]，36个字符长度的UUID不符合要求。
        ② 对MySQL索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能。

    类snowflake方案
   ![snowflake](../images/snowflake.png)

        41-bit的时间可以表示（1L<<41）/(1000L*3600*24*365)=69年的时间
        10-bit机器可以分别表示1024台机器
            如果我们对IDC划分有需求，还可以将10-bit分5-bit给IDC，分5-bit给工作机器。
            这样就可以表示32个IDC，每个IDC下可以有32台机器
        12个自增序列号可以表示2^12个ID
        理论上snowflake方案的QPS约为409.6w/s

        优点：
        毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。
        不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。
        可以根据自身业务特性分配bit位，非常灵活。

        缺点：
        强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。

    Mongdb objectID


    数据库生成
        auto_increment_increment
        auto_increment_offset
        优点：
        非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护。
        ID号单调自增，可以实现一些对ID有特殊要求的业务。
        缺点：
        强依赖DB，当DB异常时整个系统不可用，属于致命问题。
        配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。
        ID发号性能瓶颈限制在单台MySQL的读写性能。
        如果出现数据迁移可能导致灾难

   ![step-auto](../images/step-auto.png)

        系统水平扩展比较困难
        ID没有了单调递增的特性，只能趋势递增，这个缺点对于一般业务需求不是很重要，可以容忍
        数据库压力还是很大，每次获取ID都得读写一次数据库，只能靠堆机器来提高性能

    Leaf-segment数据库方案

   ![leaf segment](../images/leaf-segment.png)

        +-------------+--------------+------+-----+-------------------+-----------------------------+
        | Field       | Type         | Null | Key | Default           | Extra                       |
        +-------------+--------------+------+-----+-------------------+-----------------------------+
        | biz_tag     | varchar(128) | NO   | PRI |                   |                             |
        | max_id      | bigint(20)   | NO   |     | 1                 |                             |
        | step        | int(11)      | NO   |     | NULL              |                             |
        | desc        | varchar(256) | YES  |     | NULL              |                             |
        | update_time | timestamp    | NO   |     | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP |
        +-------------+--------------+------+-----+-------------------+-----------------------------+

        Begin
        UPDATE table SET max_id=max_id+step WHERE biz_tag=xxx
        SELECT tag, max_id, step FROM table WHERE biz_tag=xxx
        Commit

        优点：

        Leaf服务可以很方便的线性扩展，性能完全能够支撑大多数业务场景。
        ID号码是趋势递增的8byte的64位数字，满足上述数据库存储的主键要求。
        容灾性高：Leaf服务内部有号段缓存，即使DB宕机，短时间内Leaf仍能正常对外提供服务。
        可以自定义max_id的大小，非常方便业务从原有的ID方式上迁移过来。
        缺点：

        ID号码不够随机，能够泄露发号数量的信息，不太安全。
        TP999数据波动大，当号段使用完之后还是会hang在更新数据库的I/O上，tg999数据会出现偶尔的尖刺。
        DB宕机会造成整个系统不可用。

        双buffer优化

    Leaf高可用容灾


### 考虑一个业务场景: 头条的文章的评论量非常大, 比如说一篇热门文章就有几百万的评论, 设计一个后端服务, 实现评论的时序展示与分页

   [参考](https://segmentfault.com/a/1190000006499528)

   [参考](http://www.cnblogs.com/wy123/p/7003157.html)

   [参考](https://draveness.me/mysql-innodb)

    ES
    SOLR

    id递增
    不需要支持页码翻页就传 评论id + offset 翻页

    start = (page-1)*pagesize+1
    end = page*pagesize
    select * from table where id >start and id <=end

    单独维护一个数字存放

    文章表
        ID，内容，一级评论数
    评论（最多支持2级）
        ID，文章ID，评论内容，子评论数，父级评论ID（0），评论人，被评论人，（考虑是否荣誉评论人姓名防止关联查询）

    用户在看帖子的时候，一般都是查看前几页和最后几页，那么在翻页的时候最后几页的翻页查询采用 desc 的方式来实现翻页，这样就可以较好的提高性能

    select id from elastic_task_log_copy where id >= 1699999 limit 10
    ID递增，缓存分页最后一个ID
    海量数据可以一定程度忽略精确性
    可以考虑缓存行数

    使用延迟关联，即先通用limit得到需要数据的索引字段，然后再通过原表和索引字段关联获得需要数据
        select a.* from a,(select id from table_1 where is_deleted='N' limit 100000,20) b where a.id = b.id
        这种做法是不是总是生效的
        分页优化中的“INNER JOIN方式优化分页算法”到底在什么情况下会生效？
        当没有筛选条件，排序列为聚集索引（主键）时候的分页查询，所谓的分页查询优化只不过是画蛇添足
        当没有筛选条件，排序列为非聚集索引的时候，会有所改善

    从业务上实现，不分页如此多，例如只能分页前100页，后面的不允许再查了

    不使用limit N,M,而是使用limit N，即将offset转化为where条件
        select id,namefrom tableName limit5000000,10;
        select id,namefrom tableNamewhere id>5000000 limit 10;
        问题，就是有些数据会做删除，不管是逻辑删除还是物理删除，有时候筛选会去掉这种删除字段，这个时候光自增长的id是不能准确定位的，这个时候可以传入一个参数，即上次查询结果的最后一个数据的id，然后往前往后追溯
        select id,name from tableName where id>16838 and is_deleted='N' limit 10;



    2、假如用 id 翻页的方式, 数据库表如何设计? 索引如何设计?
        (文章id, 评论id) 建联合索引，评论 id 需递增


    3、假如量很大, 你觉得需要分库分表吗? 怎么分?
        需要分，分表有个权衡，按文章 id 分表，读逻辑简单，但写有热点问题；
        按评论 id 分表，读逻辑复杂，但写压力就平均了
        写是要首先保证的，而读总是有缓存等方案来折中，因此按评论 id 分表好

    4、分库分表后怎么查询分页?
        每张表查 N 条数据由 client 或 proxy merge

        对于分库分表的做分页查找
        很多情况下可能是按照时间来分页查找，多种方法可以参考（https://www.jianshu.com/p/198ee07ddd7c）
        个人比较理解文章中的禁止跳页方法。
        1、首先可以按照id，time等条件建立索引
        2、查找的sql可以是where time<#{time} and id >0 order by time desc,id asc limit #{limit},(首页，由近到远查询数据)
        3、下一页的时候其实已经知道上一页的最后时间戳time，以及id。
        那么就是where time<#{time} or （time=#{time} and id >#{id}） order by time desc,id asc，
        4、然后每个库查出部分数据，然后在中间层做排序，比如要查10条数据，那么每个库查10条，排序返回。
        参考文章中是没有time=#{time} and id >#{id}这个条件的，这是我自己加上的，是因为在一些促销，秒杀的时候，可能同一毫秒也会有多笔订单，会有跨页的情况，导致直接采用time<#{time}，会漏掉部分数据。当然如果可以接受这种不精确的情况，也可以去掉这个条件。

        参考文章中的二次查找法，其实是通过平均分摊查询数据量，虚拟最小时间，这个最小时间就是上一段的#{time}参数，
        第二次结果集既确定了时间戳和offset，也获取到了time<#{time}的结果集。这种方法就可以跳页查询了。
        （1）将order by time offset X limit Y，改写成order by time offset X/N limit Y
        （2）找到最小值time_min
        （3）between二次查询，order by time between $time_min and $time_i_max
        （4）设置虚拟time_min，找到time_min在各个分库的offset，从而得到time_min在全局的offset
        （5）得到了time_min在全局的offset，自然得到了全局的offset X limit Y

        以上是按照id取模，用户路由或者其他规则分库，那么如果按照订单的时间分库呢，是否就可以根据时间确定在哪个库了呢。

        所以还是要看具体的业务场景，查询方式来决定分库规则，再确定如果做分页查找

    5、分库分表后怎么保证主键仍然是递增的?（全局递增ID，ID预分配）

        有一张专门用于分配主键的表，每次用乐观锁的方式尝试去取一批主键过来分配，假如乐观锁失败就重试

    6、现在需要支持深分页, 页码直接跳转, 怎么实现?

        不能做精准深分页，否则压力太大，找产品进行妥协，在50或100页后数据分页是否可以不完全精确，假如可以，那么缓存深页码的起始评论 id

    7、瞬时写入量很大可能会打挂存储, 怎么保护?
        断路器

    8、断路器内部怎么实现的?

        可以用 ringbuffer

    9、断路器会造成写入失败, 假如我们不允许写入失败呢?

        先写进消息队列，削峰填谷异步落库